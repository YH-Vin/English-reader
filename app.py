import streamlit as st
from openai import OpenAI
import json
import datetime

# -----------------------------------------------------------------------------
# 1. é¡µé¢ä¸çŠ¶æ€é…ç½®
# -----------------------------------------------------------------------------
st.set_page_config(page_title="DeepRead - æ·±åº¦è‹±è¯­é™ç»´", page_icon="ğŸ§ ", layout="wide")

# åˆå§‹åŒ– Session State (ç”¨äºåƒ"å†…å­˜"ä¸€æ ·æš‚æ—¶è®°ä½æ•°æ®)
if "history" not in st.session_state:
    st.session_state.history = []  # å­˜æ”¾æ‰€æœ‰çš„é˜…è¯»è®°å½•

if "user_level" not in st.session_state:
    st.session_state.user_level = "Intermediate" # é»˜è®¤ç­‰çº§

# -----------------------------------------------------------------------------
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# -----------------------------------------------------------------------------
def get_api_key():
    # ä¼˜å…ˆä» Secrets è·å–ï¼Œå¦åˆ™ä¾§è¾¹æ è¾“å…¥
    if "ALIYUN_API_KEY" in st.secrets:
        return st.secrets["ALIYUN_API_KEY"]
    return None

def analyze_text(client, text, level, model):
    """
    è°ƒç”¨ AI è¿›è¡Œå…¨æ–¹ä½åˆ†æï¼šé™ç»´ + è¯­æ³• + è¯æ±‡ + æ–‡åŒ–
    è¦æ±‚ AI è¿”å› JSON æ ¼å¼ä»¥ä¾¿ç¨‹åºå¤„ç†
    """
    prompt = f"""
    You are an expert English teacher. Analyze the user's text based on their level: {level}.
    
    Output format: STRICT JSON with the following keys:
    1. "rewritten": The simplified version of the text (keep meaning, lower complexity).
    2. "vocabulary": A list of objects, each containing "word" (from original text), "definition" (English simple definition), and "context" (why it is used here). Max 5 hardest words.
    3. "grammar": A list of strings explaining complex sentence structures found in the original text.
    4. "culture": A string explaining any idioms, cultural references, or tone (if none, return "No special cultural context").
    
    Original Text:
    {text}
    """
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "system", "content": "You are a JSON-speaking English tutor."},
                      {"role": "user", "content": prompt}],
            temperature=0.3, # ä½æ¸©åº¦ä¿è¯ JSON æ ¼å¼ç¨³å®š
            response_format={"type": "json_object"} # å¼ºåˆ¶ JSON æ¨¡å¼ (å¦‚æœæ¨¡å‹æ”¯æŒ)
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        return {"error": str(e)}

# -----------------------------------------------------------------------------
# 3. ä¾§è¾¹æ ï¼šç”¨æˆ·ç”»åƒ (è®°å¿†åŠŸèƒ½)
# -----------------------------------------------------------------------------
with st.sidebar:
    st.header("ğŸ‘¤ å­¦ä¹ è€…æ¡£æ¡ˆ")
    
    # è·å– API Key (å¦‚æœæ²¡æœ‰é…ç½® Secretsï¼Œæ˜¾ç¤ºè¾“å…¥æ¡†)
    api_key = get_api_key()
    if not api_key:
        api_key = st.text_input("ğŸ”‘ è¾“å…¥é˜¿é‡Œäº‘ API Key", type="password")
        if not api_key:
            st.warning("è¯·è¾“å…¥ Key ä»¥å¼€å§‹ä½¿ç”¨")

    st.divider()
    
    # è®¾å®šç”¨æˆ·æ°´å¹³ (è®°å¿†åŠŸèƒ½çš„ä¸€éƒ¨åˆ†)
    st.subheader("ä½ çš„å½“å‰æ°´å¹³")
    levels = ["Beginner (å°å­¦/åˆä¸­)", "Intermediate (é«˜ä¸­/å››çº§)", "Advanced (å…­çº§/è€ƒç ”)", "Native (é›…æ€/æ‰˜ç¦)"]
    selected_level = st.selectbox("é€‰æ‹©æ°´å¹³", levels, index=1)
    st.session_state.user_level = selected_level
    
    st.info(f"ğŸ§  AI å°†æ ¹æ® **{selected_level.split()[0]}** æ°´å¹³ä¸ºä½ å®šåˆ¶å†…å®¹ã€‚")

# -----------------------------------------------------------------------------
# 4. ä¸»ç•Œé¢ï¼šåŒ Tab å¸ƒå±€
# -----------------------------------------------------------------------------
st.title("ğŸ§  DeepRead è‹±è¯­é™ç»´å­¦ä¹ å™¨")

tab1, tab2 = st.tabs(["ğŸ“– æ·±åº¦é˜…è¯» & åˆ†æ", "ğŸ–¨ï¸ èµ„æ–™åº“ & å¯¼å‡º"])

# === Tab 1: é˜…è¯»ä¸åˆ†æåŠŸèƒ½ ===
with tab1:
    col_input, col_output = st.columns([1, 1.2])
    
    with col_input:
        st.subheader("åŸæ–‡è¾“å…¥")
        source_text = st.text_area("ç²˜è´´è‹±è¯­é•¿éš¾å¥...", height=300)
        analyze_btn = st.button("ğŸš€ é™ç»´ & æ·±åº¦åˆ†æ", type="primary", use_container_width=True)
    
    with col_output:
        st.subheader("å­¦ä¹ é¢æ¿")
        result_container = st.container()

    if analyze_btn and api_key and source_text:
        with result_container:
            with st.spinner("AI æ­£åœ¨æ‹†è§£è¯­æ³•ã€æŸ¥è¯ã€é‡å†™ä¸­..."):
                client = OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
                
                # è°ƒç”¨æ ¸å¿ƒåˆ†æå‡½æ•°
                data = analyze_text(client, source_text, st.session_state.user_level, "qwen-plus")
                
                if "error" in data:
                    st.error(f"åˆ†æå¤±è´¥: {data['error']}")
                else:
                    # 1. å±•ç¤ºé™ç»´æ–‡æœ¬
                    st.success("âœ… é™ç»´æ”¹å†™")
                    st.markdown(f"**{data['rewritten']}**")
                    
                    # 2. å±•ç¤ºåˆ†æ (ä½¿ç”¨æŠ˜å é¢æ¿ä¿æŒæ•´æ´)
                    with st.expander("ğŸ” é‡ç‚¹è¯æ±‡ (Vocabulary)", expanded=True):
                        for v in data.get('vocabulary', []):
                            st.markdown(f"- **{v['word']}**: {v['definition']}")
                    
                    with st.expander("ğŸ“ è¯­æ³•æ‹†è§£ (Grammar)"):
                        for g in data.get('grammar', []):
                            st.markdown(f"- {g}")
                            
                    with st.expander("ğŸŒ æ–‡åŒ–ä¸èƒŒæ™¯ (Context)"):
                        st.write(data.get('culture', 'æ— ç‰¹æ®ŠèƒŒæ™¯'))

                    # 3. å­˜å…¥å†å²è®°å½• (è®°å¿†åŠŸèƒ½)
                    record = {
                        "time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M"),
                        "original": source_text,
                        "rewritten": data['rewritten'],
                        "vocab": data.get('vocabulary', []),
                        "grammar": data.get('grammar', [])
                    }
                    st.session_state.history.insert(0, record) # æ’åˆ°æœ€å‰é¢

# === Tab 2: æ±‡æ€»ä¸å¯¼å‡ºåŠŸèƒ½ ===
with tab2:
    st.header("ğŸ—‚ï¸ ä½ çš„å­¦ä¹ èµ„æ–™åº“")
    
    if not st.session_state.history:
        st.info("è¿˜æ²¡æœ‰è®°å½•ï¼Œå¿«å» Tab 1 è¿›è¡Œé˜…è¯»å§ï¼")
    else:
        # å¤šé€‰æ¡†ï¼šé€‰æ‹©è¦æ‰“å°çš„å†…å®¹
        st.write("å‹¾é€‰ä½ æƒ³è¦æ±‡æ€»æ‰“å°çš„ç¬”è®°ï¼š")
        
        # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥ä¿å­˜è¢«é€‰ä¸­çš„ç´¢å¼•
        selected_indices = []
        
        for i, item in enumerate(st.session_state.history):
            with st.container(border=True):
                # checkbox çš„ key å¿…é¡»å”¯ä¸€
                is_selected = st.checkbox(f"{item['time']} - {item['original'][:30]}...", key=f"hist_{i}")
                if is_selected:
                    selected_indices.append(i)
                
                st.caption(f"é™ç»´: {item['rewritten'][:50]}...")

        st.divider()
        
        # å¯¼å‡ºé€»è¾‘
        if selected_indices:
            st.subheader("ğŸ“¤ å¯¼å‡ºé€‰é¡¹")
            
            # ç”Ÿæˆ Markdown æ ¼å¼çš„æ–‡æœ¬ (æœ€é€‚åˆæ‰“å°å’Œæ’ç‰ˆ)
            export_text = f"# è‹±è¯­å­¦ä¹ æ±‡æ€» ({datetime.datetime.now().strftime('%Y-%m-%d')})\n\n"
            for idx in selected_indices:
                note = st.session_state.history[idx]
                export_text += f"## ğŸ“… è®°å½•: {note['time']}\n"
                export_text += f"### 1. åŸæ–‡\n> {note['original']}\n\n"
                export_text += f"### 2. é™ç»´ç‰ˆ\n{note['rewritten']}\n\n"
                export_text += f"### 3. æ ¸å¿ƒè¯æ±‡\n"
                for v in note['vocab']:
                    export_text += f"- **{v['word']}**: {v['definition']}\n"
                export_text += f"\n### 4. è¯­æ³•è§£æ\n"
                for g in note['grammar']:
                    export_text += f"- {g}\n"
                export_text += "\n---\n\n"

            # ä¸‹è½½æŒ‰é’®
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ Markdown è®²ä¹‰ (å¯ç›´æ¥æ‰“å°)",
                data=export_text,
                file_name="english_study_notes.md",
                mime="text/markdown"
            )
        else:
            st.caption("è¯·å…ˆå‹¾é€‰ä¸Šé¢çš„è®°å½•ä»¥è¿›è¡Œå¯¼å‡ºã€‚")
