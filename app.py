import streamlit as st
from openai import OpenAI
import json
import datetime
import base64
import zlib

# -----------------------------------------------------------------------------
# 1. é¡µé¢ä¸è§†è§‰é…ç½®
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="DeepRead Pro",
    page_icon="ğŸ“˜",
    layout="wide",
    initial_sidebar_state="expanded"
)  # <--- è¿™é‡Œå¿…é¡»é—­åˆï¼è®¾ç½®ç»“æŸã€‚

# CSS: å…¼å®¹æ·±è‰²æ¨¡å¼ï¼Œä¿®å¤è¾“å…¥æ¡†èƒŒæ™¯é—®é¢˜
custom_css = """
<style>
    .stApp { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; }
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    h1 { font-weight: 700 !important; letter-spacing: -0.03em !important; }
    div.stButton > button { border-radius: 10px !important; font-weight: 600 !important; border: none !important; padding: 0.5rem 1rem !important; transition: transform 0.1s; }
    div.stButton > button:active { transform: scale(0.98); }
    hr { margin: 2em 0 !important; border: none !important; border-top: 1px solid #eaeaea !important; }
    .mobile-alert { background-color: #fff0f0; padding: 12px; border-radius: 8px; border-left: 5px solid #ff4b4b; margin-bottom: 25px; color: #333; }
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# -----------------------------------------------------------------------------
# 2. çŠ¶æ€ç®¡ç†ä¸å›è°ƒå‡½æ•°
# -----------------------------------------------------------------------------
if "history" not in st.session_state:
    st.session_state.history = []
if "user_level" not in st.session_state:
    st.session_state.user_level = "Intermediate (B1-B2)"

# --- å…³é”®ä¿®å¤ï¼šæ¸…ç©ºæ–‡æœ¬çš„å›è°ƒå‡½æ•° ---
def clear_text():
    # å°†è¾“å…¥æ¡†ç»‘å®šçš„ key çš„å€¼è®¾ä¸ºç©º
    st.session_state["source_input"] = ""

# -----------------------------------------------------------------------------
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°åº“
# -----------------------------------------------------------------------------
def get_api_key():
    try:
        if "ALIYUN_API_KEY" in st.secrets:
            return st.secrets["ALIYUN_API_KEY"]
    except FileNotFoundError: pass 
    return None

def generate_save_token(data):
    if not data: return ""
    try:
        json_str = json.dumps(data)
        compressed = zlib.compress(json_str.encode('utf-8'))
        return base64.b64encode(compressed).decode('utf-8')
    except: return ""

def load_save_token(token):
    try:
        decoded = base64.b64decode(token)
        json_str = zlib.decompress(decoded).decode('utf-8')
        return json.loads(json_str)
    except: return None

def analyze_text_pro(client, text, level, model):
    """
    æè‡´ä¼˜åŒ–ç‰ˆåˆ†æå‡½æ•°ï¼š
    1. é€å¥å…¨é‡è§£æ (Sentence-by-Sentence Breakdown)
    2. ä¸¥æ ¼çš„è¯æ±‡åˆ†çº§è¿‡æ»¤ (Strict Vocabulary Filtering)
    3. é™ç»´é‡Šä¹‰ (Simplified Definitions)
    """
    
    # æå–ç­‰çº§çš„æ ¸å¿ƒå…³é”®è¯ï¼ˆä¾‹å¦‚ "Intermediate"ï¼‰ï¼Œç”¨äº Prompt ä¸­çš„å¼ºè°ƒ
    level_keyword = level.split()[0] if level else "Intermediate"

    prompt = f"""
    You are a strict and elite Linguistics Professor and Curriculum Designer. 
    Analyze the provided English text for a student at the CEFR level: {level}.

    Your task is to generate a structured learning guide. You must strictly adhere to the following rules:

    ### RULE 1: EXPLANATION (MANDATORY Sentence-by-Sentence Analysis)
    - You MUST iterate through the text **sentence by sentence**.
    - For **EVERY** sentence in the text, provide a breakdown object.
    - **DO NOT SKIP** any sentence unless it is essentially meaningless (e.g., just "Yes." or "No.").
    - If a sentence is simple, briefly explain its function or connection to the context.
    - If a sentence is complex, deconstruct its logic deeply.
    - **"title"** should be a short concept summary (e.g., "The Opening Argument", "Supporting Detail", "The Conclusion").
    - **"original"** must be the exact sentence from the text.

    ### RULE 2: VOCABULARY (Strict Level Filtering)
    - **SELECTION CRITERIA**: Select words that are **significantly difficult** for a learner at {level}.
    - **NEGATIVE CONSTRAINT**: DO NOT select words that a student at {level} should already know. If the word is common (e.g., "apple", "book", "difficult" for Intermediate), **IGNORE IT**.
    - If there are no words above the user's level, return an empty list for vocabulary. Do not fill it with easy words just to fill space.
    - **Max quantity**: 6 words (but only if they are truly hard).

    ### RULE 3: DEFINITIONS (Comprehensible Input)
    - The **"definition"** and **"context"** for vocabulary must be written using English that is **simpler** than the user's current level ({level}).
    - Do not use complex words to explain other complex words.

    ### OUTPUT FORMAT
    Return a **STRICT JSON** object with the following keys:
    1. "main_idea": Concise summary (2-3 sentences).
    2. "explanation": A list of objects. Each object must represent ONE sentence from the text. Keys: "title", "original", "meaning".
    3. "grammar": List of objects (keys: "title", "original", "breakdown"). Focus on syntax relevant to {level}.
    4. "vocabulary": List of objects (keys: "word", "ipa", "definition", "context").

    Original Text:
    {text}
    """
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                # System Prompt å¼ºåŒ–äººè®¾ï¼Œç¡®ä¿ AI å¤„äºä¸¥æ ¼æ¨¡å¼
                {"role": "system", "content": f"You are a strict English tutor. You never hallucinate. You prioritize the user's CEFR level ({level_keyword}) above all else."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1, # æä½æ¸©åº¦ï¼Œé˜²æ­¢ AI å‘æ•£æˆ–é—æ¼ï¼Œä¿è¯é€»è¾‘ä¸¥å¯†
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        return {"error": str(e)}

def analyze_text_stream(client, text, level, model):
    """
    æµå¼åˆ†æå‡½æ•°ï¼šç›´æ¥è¦æ±‚ AI è¾“å‡º Markdown æ ¼å¼ï¼Œä¸å†ç»è¿‡ JSON è§£æ
    """
    # æå–ç­‰çº§å…³é”®è¯
    level_keyword = level.split()[0] if level else "Intermediate"

    prompt = f"""
    You are a strict and elite Linguistics Professor. Analyze the text for a student at CEFR level: {level}.
    
    ### STRICT OUTPUT RULES
    1. Output **DIRECTLY in Markdown format**. Do not use JSON.
    2. **Sentence-by-Sentence Analysis**: You MUST iterate through the text sentence by sentence. Do not skip any sentence.
    3. **Vocabulary**: Select words significantly difficult for {level}. Definitions must be simpler than the word itself.
    
    ### REQUIRED MARKDOWN STRUCTURE
    Please output exactly in this format:

    ### **Main Idea**
    [Concise summary in 2-3 sentences]

    ---
    ### **Detailed Explanation**
    **1. [Short Concept Title]**
    > *Original: "[The exact sentence]"*
    *   **Meaning:** [Deep analysis of the sentence logic]

    **2. [Next Concept Title]**
    ... (Repeat for EVERY sentence) ...

    ---
    ### **Grammar Breakdown**
    **1. [Grammar Point Title]**
    > *"[Snippet]"*
    *   **Analysis:** [Syntax breakdown]

    ---
    ### **Vocabulary**
    **1. [Word]** `[IPA]`
    *   **Def:** [Simple definition]
    *   **Ctx:** [Context usage]

    Original Text:
    {text}
    """
    
    # å¼€å¯ stream=True
    stream = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a specialized Linguistics Tutor. Output strictly structured Markdown."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1,
        stream=True  # å…³é”®ç‚¹ï¼šå¼€å¯æµå¼
    )
    return stream

# -----------------------------------------------------------------------------
# 4. ä¾§è¾¹æ 
# -----------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ Settings")
    api_key = get_api_key()
    if not api_key:
        api_key = st.text_input("ğŸ”‘ API Key", type="password", placeholder="Paste Aliyun Key here")
        if not api_key: st.warning("âš ï¸ è¯·è¾“å…¥ Key ä»¥ä½¿ç”¨")
    st.divider()
    st.subheader("ğŸ‘¤ User Level")
    st.session_state.user_level = st.selectbox("Select Target Level:", ["Beginner (A1-A2)", "Intermediate (B1-B2)", "Advanced (C1-C2)"], index=1)
    st.divider()
    with st.expander("ğŸ’¾ Backup / Restore", expanded=False):
        st.caption("é˜²æ­¢æ•°æ®ä¸¢å¤±ï¼Œè¯·å®šæœŸå¤‡ä»½")
        if st.session_state.history:
            token = generate_save_token(st.session_state.history)
            st.code(token, language="text")
            st.caption("ğŸ‘† å…¨é€‰å¤åˆ¶ï¼Œå­˜å…¥å¤‡å¿˜å½•ã€‚")
        else: st.info("æš‚æ— è®°å½•å¯å¯¼å‡º")
        st.markdown("---")
        restore_token = st.text_input("Import Token:", placeholder="Paste token here...", label_visibility="collapsed")
        if st.button("ğŸ”„ Restore Data", use_container_width=True):
            data = load_save_token(restore_token)
            if data:
                st.session_state.history = data
                st.toast("âœ… æ•°æ®æ¢å¤æˆåŠŸï¼", icon="ğŸ‰")
                st.rerun()
            else: st.error("æ— æ•ˆçš„å­˜æ¡£ç ")

# -----------------------------------------------------------------------------
# 5. ä¸»ç•Œé¢
# -----------------------------------------------------------------------------

# è·å–å½“å‰æ—¥æœŸ
today_str = datetime.datetime.now().strftime("%Y-%m-%d %A")

# å·¦å³å¸ƒå±€ï¼šå·¦æ ‡é¢˜ï¼Œå³æ—¥æœŸ
col_title, col_date = st.columns([3, 1])

with col_title:
    st.title("ğŸ“˜ DeepRead Pro")

with col_date:
    # å³å¯¹é½æ˜¾ç¤ºæ—¥æœŸ
    st.markdown(f"<div style='text-align: right; color: gray; padding-top: 25px; font-size: 0.9em;'>ğŸ“… {today_str}</div>", unsafe_allow_html=True)

st.caption("Your AI-Powered Linguistics Tutor")


tab_analysis, tab_library = st.tabs(["âœ¨ Deep Analysis", "ğŸ“š My Library"])

# === Tab 1: æ·±åº¦åˆ†æ ===
with tab_analysis:
    st.markdown("""<div class="mobile-alert"><strong style='color: #d8000c;'>âš ï¸ æ‰‹æœºç”¨æˆ·è¯·æ³¨æ„ï¼š</strong><span style='color: #333;'>è¯·ç‚¹å‡»å·¦ä¸Šè§’ <b>></b> ç®­å¤´å±•å¼€è®¾ç½®ï¼Œæˆ–åœ¨æµè§ˆå™¨èœå•é€‰æ‹©<b>â€œè¯·æ±‚æ¡Œé¢ç½‘ç«™â€</b>ä»¥è·å¾—æœ€ä½³ä½“éªŒã€‚</span></div>""", unsafe_allow_html=True)

    col_in, col_out = st.columns([1, 1.1])
    
    with col_in:
        st.markdown("#### Input Text")
        
        # --- å…³é”®ä¿®å¤ï¼šæ·»åŠ  key å‚æ•° ---
        source_text = st.text_area(
            "Content", 
            height=350, 
            label_visibility="collapsed",
            key="source_input"  # ç»™ç»„ä»¶ä¸€ä¸ªIDï¼Œæ–¹ä¾¿å›è°ƒå‡½æ•°æ‰¾åˆ°å®ƒ
        )
        
        c1, c2, c3 = st.columns([1.5, 1, 1])
        with c1:
            model = st.selectbox("Model", ["qwen3-max", "qwen-flash"], label_visibility="collapsed")
        with c2:
            # --- å…³é”®ä¿®å¤ï¼šç»‘å®š on_click å›è°ƒ ---
            st.button("ğŸ—‘ï¸ Clear", use_container_width=True, on_click=clear_text)
        with c3:
            analyze_btn = st.button("Analyze ğŸš€", type="primary", use_container_width=True)

    with col_out:
        st.markdown("#### Insight")
        result_box = st.container()

    # -------------------------------------------------------------------------
    # æ›¿æ¢åçš„æµå¼è¾“å‡ºé€»è¾‘
    # -------------------------------------------------------------------------
    if analyze_btn:
        if not api_key:
            st.toast("ğŸš« Please enter API Key first.", icon="ğŸ”’")
        elif not source_text:
            st.toast("âœï¸ Please paste some text.", icon="ğŸ“")
        else:
            with result_box:
                # 1. å‡†å¤‡ä¸€ä¸ªç©ºçš„å®¹å™¨ç”¨æ¥æ˜¾ç¤ºæµå¼å†…å®¹
                placeholder = st.empty()
                full_response = ""
                
                # 2. å¼€å§‹è°ƒç”¨æµå¼å‡½æ•°
                try:
                    client = OpenAI(
                        api_key=api_key, 
                        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                    )
                    
                    # è°ƒç”¨åˆšæ‰æ–°å†™çš„ stream å‡½æ•°
                    stream = analyze_text_stream(client, source_text, st.session_state.user_level, model)
                    
                    # 3. å®æ—¶å¾ªç¯æ¥æ”¶æ•°æ®å¹¶æ˜¾ç¤º
                    for chunk in stream:
                        content = chunk.choices[0].delta.content
                        if content:
                            full_response += content
                            # å®æ—¶åˆ·æ–° UIï¼ŒåŠ ä¸€ä¸ªå…‰æ ‡ â–Œ è®©å®ƒçœ‹èµ·æ¥åƒåœ¨æ‰“å­—
                            placeholder.markdown(full_response + "â–Œ")
                    
                    # 4. ç”Ÿæˆå®Œæ¯•ï¼Œç§»é™¤å…‰æ ‡ï¼Œæ˜¾ç¤ºæœ€ç»ˆç»“æœ
                    placeholder.markdown(full_response)
                    
                    # 5. å­˜å…¥å†å²è®°å½• (æ³¨æ„ï¼šè¿™é‡Œç›´æ¥å­˜ Markdown å­—ç¬¦ä¸²)
                    record = {
                        "time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M"),
                        "original": source_text,
                        "markdown": full_response  # ç›´æ¥å­˜ç”Ÿæˆå¥½çš„ Markdown
                    }
                    st.session_state.history.insert(0, record)
                    st.toast("âœ… Analysis complete & Saved!", icon="ğŸ’¾")

                except Exception as e:
                    st.error(f"Connection Error: {e}")

# === Tab 2: å†å²èµ„æ–™åº“ ===
with tab_library:
    st.markdown("#### ğŸ—‚ï¸ Knowledge Base")
    if not st.session_state.history: st.info("No records found.")
    else:
        col_tools, _ = st.columns([1, 4])
        with col_tools:
            if st.button("ğŸ—‘ï¸ Clear All History", type="secondary"):
                st.session_state.history = []
                st.rerun()
        st.divider()
        selected_records = []
        for i, note in enumerate(st.session_state.history):
            with st.container():
                c_check, c_content = st.columns([0.05, 0.95])
                with c_check:
                    if st.checkbox("", key=f"check_{i}"): selected_records.append(note)
                with c_content:
                    with st.expander(f"ğŸ“… {note['time']} - {note['original'][:50]}..."): st.markdown(note['markdown'])
            st.divider()
        if selected_records:
            st.success(f"Selected {len(selected_records)} notes.")
            final_export = f"# DeepRead Study Notes\nGenerated: {datetime.datetime.now().strftime('%Y-%m-%d')}\n\n"
            for note in selected_records: final_export += f"## Record: {note['time']}\n{note['markdown']}\n\n========================================\n\n"
            st.download_button("ğŸ“¥ Download Markdown", final_export, f"DeepRead_Notes_{datetime.datetime.now().strftime('%Y%m%d')}.md", "text/markdown", type="primary")

# --- åº•éƒ¨ï¼šå›ºå®šç‰ˆæƒç½²å (CSS) ---
st.markdown(
    """
    <style>
        .footer {
            position: fixed;
            left: 0;
            bottom: 0;
            width: 100%;
            background-color: transparent; /* é€æ˜èƒŒæ™¯ */
            color: #888; /* ç°è‰²æ–‡å­— */
            text-align: center;
            padding: 10px;
            font-size: 12px;
            z-index: 999;
            pointer-events: none; /* è®©é¼ æ ‡å¯ä»¥ç©¿é€æ–‡å­—ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’® */
        }
    </style>
    <div class="footer">
        Designed by <b>uncompleted vin</b> | Powered by Aliyun Qwen
    </div>
    """, 
    unsafe_allow_html=True
)
